# -*- coding: utf-8 -*-
"""LanguageSCM-VAE.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A0wIqtAr8wddNCPGGnf9-RZJ2H6xtAx2
"""

!pip install pytorch-lightning==1.8.4

import os
import argparse
import json
import torch
import torch.utils.data as data
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pathlib import Path
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.utilities.seed import seed_everything
import pickle
import pandas as pd

features_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/polarity_scores_df', sep='\t')

features_df = features_df.sample(frac = 1)
features_df.reset_index(drop=True).head()

split = int(0.8*len(features_df))
train_data = features_df.iloc[:split,:]
test_data = features_df.iloc[split:,:]

len(test_data)

train_data.head()

test_data.head()

demo_train_data = train_data[0:10]
demo_test_data = test_data[0:10]

train_data.to_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/train_data', sep='\t')
# valid_data.to_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/val_data', sep='\t')
test_data.to_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/test_data', sep='\t')

demo_train_data.to_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/demo_train_data', sep='\t')
demo_test_data.to_csv('/content/drive/My Drive/Colab Notebooks/causality/dataset/demo_test_data', sep='\t')

from torchvision import transforms
import numpy as np
from PIL import Image
from nltk.tokenize import TweetTokenizer
from collections import defaultdict, Counter, OrderedDict
import io
from torch.utils.data import Dataset
class OrderedCounter(Counter, OrderedDict):
    """Counter that remembers the order elements are first encountered"""
    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, OrderedDict(self))

    def __reduce__(self):
        return self.__class__, (OrderedDict(self),)
class LanguageData(Dataset):
	def __init__(self, max_sequence_length, data_dir, dataset="train"): #train, test, val

		self.dataset = dataset
		self.data_dir = data_dir
		self.raw_data_path = self.data_dir + "/" +self.dataset + '_data'
		self.raw_data = pd.read_csv(self.raw_data_path, sep='\t')
		# print("1")
		self.text = [d['text'] for idx, d in self.raw_data.iterrows()]
		# print("2")
		self.textlabel = [[d['topic'], d['rate'], d['neg'], d['neu'], d['pos']] for idx, d in self.raw_data.iterrows()]
		# print("3")
		self.vocab_file = 'lang.vocab.json'
		self.data_file = 'lang' + dataset + '.json'
		self.min_occ = 1
		self.max_sequence_length = max_sequence_length

		self.create_data()
		# print("4")
		self.load_data()

	@property
	def vocab_size(self):
			return len(self.w2i)

	@property
	def pad_idx(self):
			return self.w2i['<pad>']

	@property
	def sos_idx(self):
			return self.w2i['<sos>']

	@property
	def eos_idx(self):
			return self.w2i['<eos>']

	@property
	def unk_idx(self):
			return self.w2i['<unk>']

	def get_w2i(self):
			return self.w2i

	def get_i2w(self):
			return self.i2w
	def load_vocab(self):
		with open(os.path.join(self.data_dir, self.vocab_file), 'r') as vf:
				vocab = json.load(vf)

		return vocab


	def create_vocab(self):

		tokenizer = TweetTokenizer(preserve_case=False)

		w2c = OrderedCounter()
		w2i = dict()
		i2w = dict()

		special_tokens = ['<pad>', '<unk>', '<sos>', '<eos>']
		for st in special_tokens:
				i2w[len(w2i)] = st
				w2i[st] = len(w2i)

		for idx, d in self.raw_data.iterrows():
			text = d['text']
			# print(text)
			words = tokenizer.tokenize(text)
			w2c.update(words)
		for w, c in w2c.items():
			if c > self.min_occ and w not in special_tokens:
				i2w[len(w2i)] = w
				w2i[w] = len(w2i)
		vocab = dict(w2i=w2i, i2w=i2w)
		with io.open(os.path.join(self.data_dir, self.vocab_file), 'wb') as vf:
				data = json.dumps(vocab, ensure_ascii=False)
				vf.write(data.encode('utf8', 'replace'))

	def create_data(self):
		# print("5")
		if self.dataset == 'train':
				self.create_vocab()
		# print("6")
		vocab = self.load_vocab()
		# print("7")
		self.w2i, self.i2w = vocab['w2i'], vocab['i2w']

		tokenizer = TweetTokenizer(preserve_case=False)

		data = defaultdict(dict)
		for idx, d in self.raw_data.iterrows():

			topic = d['topic']
			rate = d['rate']
			neg = d['neg']
			neu = d['neu']
			pos = d['pos']
			label = [topic, rate, neg, neu, pos]
			text = d['text']
			words = tokenizer.tokenize(text)

			input = ['<sos>'] + words
			input = input[:self.max_sequence_length]

			target = words[:self.max_sequence_length-1]
			target = target + ['<eos>']

			assert len(input) == len(target), "%i, %i"%(len(input), len(target))
			length = len(input)

			input.extend(['<pad>'] * (self.max_sequence_length-length))
			target.extend(['<pad>'] * (self.max_sequence_length-length))

			input = [self.w2i.get(w, self.w2i['<unk>']) for w in input]
			target = [self.w2i.get(w, self.w2i['<unk>']) for w in target]

			id = len(data)
			data[id]['input'] = input
			data[id]['target'] = target
			data[id]['length'] = length
			data[id]['label'] = label

		with io.open(os.path.join(self.data_dir, self.data_file), 'wb') as data_file:
				data = json.dumps(data, ensure_ascii=False)
				data_file.write(data.encode('utf8', 'replace'))


	def load_data(self,  vocab=True):

		with open(os.path.join(self.data_dir, self.data_file), 'r') as file:
				self.data = json.load(file)
		if vocab:
			with open(os.path.join(self.data_dir, self.vocab_file), 'r') as file:
					vocab = json.load(file)
			self.w2i, self.i2w = vocab['w2i'], vocab['i2w']


	def __getitem__(self, idx):
		idx = str(idx)

		return {
				'input': np.asarray(self.data[idx]['input']),
				'target': np.asarray(self.data[idx]['target']),
				'length': self.data[idx]['length'],
				'label' : torch.from_numpy(np.asarray(self.data[idx]['label']))
		}


	def __len__(self):
		return len(self.data)

#data, max_sequence_length, data_dir, dataset="train"
max_sequence_length = 50
data_dir = '/content/drive/My Drive/Colab Notebooks/causality/dataset'
train_dataset = LanguageData(max_sequence_length, data_dir, "train")
#train_dataset.data['0']['target']

# val_dataset = LanguageData(max_sequence_length, data_dir, "val")
test_dataset = LanguageData(max_sequence_length, data_dir, "test")

demo_train_dataset = LanguageData(max_sequence_length, data_dir, "demo_train")
demo_test_dataset = LanguageData(max_sequence_length, data_dir, "demo_test")

import torchvision.utils as vutils
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data as data
import pytorch_lightning as pl
from pytorch_lightning.callbacks import LearningRateMonitor
import numpy as np
import os
from collections import OrderedDict, defaultdict
import sys
import random
class Causal_LSTM_VAE(torch.nn.Module):

  def __init__(self, model_name,vocab_size, embed_size, hidden_size, latent_size,  batch_size, z1_dim= 5, z2_dim=5, num_layers=1, lr=0.0001,alpha=0.3, beta=1, gamma=1, lambda_v=0.0001, inference=False, use_causal_prior=True, warmup=100, max_iters=100000, **kwargs):
    super().__init__()
    # print('init')

    self.device = "cuda" if torch.cuda.is_available() else "cpu"
    self.scale = np.array([[0,5],[0,5],[0.0, 1.0],[0.0,1.0],[0.0,1.0]])
    self.vocab_size = vocab_size
    self.embed_size = embed_size
    self.hidden_size = hidden_size
    self.latent_size = latent_size
    self.num_layers = num_layers
    self.lstm_factor = num_layers
    self.z1_dim = z1_dim
    self.z2_dim = z2_dim
    self.batch_size = batch_size
    self.causal_matrix = torch.zeros(z1_dim, z1_dim)
    self.causal_matrix[0, 2:5] = 1
    self.causal_matrix[1, 2:5] = 1

    self.net = nn.Sequential(
            nn.Linear(self.latent_size, 32),
            nn.ELU(),
            nn.Linear(32, self.z1_dim),
        )

    self.dictionary = LanguageData(data_dir='/content/drive/My Drive/Colab Notebooks/causality/dataset', dataset="train",  max_sequence_length= 50)
    self.embed = torch.nn.Embedding(num_embeddings= self.vocab_size,embedding_dim= self.embed_size)
    # print('encoder')

    # Encoder Part
    self.encoder_lstm = torch.nn.LSTM(input_size= self.embed_size,hidden_size= self.hidden_size, batch_first=True, num_layers= self.num_layers)
    self.mean = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, out_features= self.latent_size)
    self.log_variance = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, out_features= self.latent_size)

    # Decoder Part
    # print('decoder')
    self.init_hidden_decoder = torch.nn.Linear(in_features= self.latent_size, out_features= self.hidden_size * self.lstm_factor)
    self.decoder_lstm = torch.nn.LSTM(input_size= self.embed_size, hidden_size= self.hidden_size, batch_first = True, num_layers = self.num_layers)
    self.output = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, out_features= self.vocab_size)
    self.log_softmax = torch.nn.LogSoftmax(dim=2)


  def init_hidden(self, batch_size):
    hidden_cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)
    state_cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)
    return (hidden_cell, state_cell)

  def mask_non_parents(self, matrix, i):

    I = torch.eye((5))

    masked_matrix = torch.mul((self.causal_matrix + I)[:, i].reshape(self.z1_dim, 1).clone(), matrix.clone())
    return masked_matrix

  def add_nn_layer(self, z):
        rx = self.net(z)
        return rx

  def encoder(self, packed_x_embed,total_padding_length, hidden_encoder, label=None, mask=None, value=None):
    # print("encode")
    # pad the packed input.

    packed_output_encoder, hidden_encoder = self.encoder_lstm(packed_x_embed, hidden_encoder)
    output_encoder, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_encoder, batch_first=True, total_length= total_padding_length)

    # Extimate the mean and the variance of q(z|x)
    mean = self.mean(hidden_encoder[0])
    log_var = self.log_variance(hidden_encoder[0])


    #print('mu sigma sizes:',mean.size(),log_var.size()) === torch.Size([1, 32, 16])

    ## ADD SCM ##
    # print('mu sigma sizes:',mean.size(),log_var.size())
    mean = mean.reshape([mean.size()[0], mean.size()[1], self.z1_dim, self.z2_dim])
    log_var = log_var.reshape([log_var.size()[0], log_var.size()[1], self.z1_dim, self.z2_dim])
    # print('mu sigma sizes:',mean.size(),log_var.size())

    mean_temp = torch.clone(mean)
    for i in range(5):
      if i == 0 or i == 1:
        # no changes to z  temp
        mean_temp[:, :, i, :] = mean[:, :, i, :] # row ith : each row has 5 (z2) columns
      else:
        masked_matrix = self.mask_non_parents(mean_temp, i).reshape([mean_temp.size()[0], mean_temp.size()[1], self.latent_size])
        mean_temp[:, :, i, :] = self.add_nn_layer(masked_matrix).reshape([mean.size()[0], mean.size()[1], self.z1_dim]).to(device)

      if mask == i:
        print('mask=yes')
        mean_temp[:, :, mask, :], log_var[:, :, mask, :] = self.perform_intervention(mean_temp, log_var, mask, label, value)

    mean_masked = mean_temp

    # print('mu sigma NEW sizes:',mean_masked.size())

    # Generate a unit gaussian noise.
    batch_size = output_encoder.size(0)
    seq_len = output_encoder.size(1)

    std = torch.exp(0.5 * log_var)   # e^(0.5 log_var) = var^0.5
    noise = torch.randn(mean_masked.size()).to(self.device)
    old_noise = torch.randn(batch_size, self.latent_size).to(self.device)

    z = noise * std + mean

    z_masked = noise * std + mean_masked

    # print('z sizes:',z.size())
    # print('z masked sizes:',z_masked.size())

    return z, z_masked, mean, mean_masked, log_var, hidden_encoder


  def perform_intervention(self, z_temp, z_v, mask, label, value):
    print('perform_intervention')
    label[:, mask] = value
    cp_m, cp_v = self.condition_prior(self.scale, label, self.z1_dim)
    z_temp[:, :, mask, :] = cp_m[:, mask, :].to(device)
    z_v[:, :, mask, :] = torch.abs(cp_v[:, mask, :])

    return z_temp[:, :, mask, :], z_v[:, :, mask, :]

  def condition_prior(self, scale, label, dim):
    mean = torch.ones(label.size()[0], label.size()[1], dim)
    var = torch.ones(label.size()[0], label.size()[1], dim)
    # print("label = ", label.size())
    # print("mean = ", mean.size())
    # print("label", label)
    for i in range(label.size()[0]):
      for j in range(label.size()[1]):
        # print(label[i][j])
        mul = (float(label[i][j]) - scale[j][0]) / (scale[j][1] - 0)
        mean[i][j] = torch.ones(dim) * mul
        var[i][j] = torch.ones(dim) * 1
    return mean, var

  def decoder(self, z, packed_x_embed, total_padding_length=None):
    # print("decode")
    # print("z shape in decoder", z.size())
    # reshape the latent matrix
    z = z.view(1, self.batch_size, self.latent_size)
    # print("z shape in decoder NEW", z.size())

    hidden_decoder = self.init_hidden_decoder(z)
    hidden_decoder = (hidden_decoder, hidden_decoder)

    # pad the packed input.
    packed_output_decoder, hidden_decoder = self.decoder_lstm(packed_x_embed,hidden_decoder)
    output_decoder, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_decoder, batch_first=True, total_length= total_padding_length)


    x_hat = self.output(output_decoder)

    x_hat = self.log_softmax(x_hat)


    return x_hat

  def get_embedding(self, x):
    x_embed = self.embed(x)

    # Total length for pad_packed_sequence method = maximum sequence length
    maximum_sequence_length = x_embed.size(1)

    return x_embed, maximum_sequence_length

  def forward(self, x,sentences_length,hidden_encoder, label=None, mask=None, value=None):

    """
      x : bsz * seq_len

      hidden_encoder: ( num_lstm_layers * bsz * hidden_size, num_lstm_layers * bsz * hidden_size)

    """
    # Get Embeddings
    x_embed, maximum_padding_length = self.get_embedding(x)

    # Packing the input
    packed_x_embed = torch.nn.utils.rnn.pack_padded_sequence(input= x_embed, lengths= sentences_length, batch_first=True, enforce_sorted=False)


    # Encoder
    z, z_masked, mean, mean_masked, log_var, hidden_encoder = self.encoder(packed_x_embed, maximum_padding_length, hidden_encoder, label, mask, value)
    # z, mean, log_var, hidden_encoder = self.encoder(packed_x_embed, maximum_padding_length, hidden_encoder)

    # Decoder
    x_hat = self.decoder(z_masked, packed_x_embed, maximum_padding_length)

    return x_hat, mean, mean_masked, log_var, z, z_masked, hidden_encoder, self.causal_matrix

  def inference(self, n_samples, sos, z):

      # generate random z
      batch_size = 1
      seq_len = 1
      idx_sample = []


      input = torch.Tensor(1, 1).fill_(self.dictionary.get_w2i()[sos]).long().to(self.device)

      hidden = self.init_hidden_decoder(z)
      hidden = (hidden, hidden)

      for i in range(n_samples):
        input = self.embed(input)
        # print("input size", input.size())
        output,hidden = self.decoder_lstm(input, hidden)
        output = self.output(output)
        output = self.log_softmax(output)
        output = output.exp()
        # print("output", output)
        # print("output size", output.size())
        _, s = torch.topk(output, 1)
        # print("s size", s.size())
        idx_sample.append(s.item())
        input = s.squeeze(0)

      w_sample = [self.dictionary.get_i2w()[str(idx)] for idx in idx_sample]
      # print("idx", idx_sample)
      w_sample = " ".join(w_sample)
      return w_sample

import torch

class VAE_Loss(torch.nn.Module):

  def __init__(self):
    super(VAE_Loss, self).__init__()

    self.nlloss = torch.nn.NLLLoss()

  def kl_normal(self, qm, qv, pm, pv):
    """
    Computes the elem-wise KL divergence between two normal distributions KL(q || p) and
    sum over the last dimension

    Args:
      qm: tensor: (batch, dim): q mean
      qv: tensor: (batch, dim): q variance
      pm: tensor: (batch, dim): p mean
      pv: tensor: (batch, dim): p variance

    Return:
      kl: tensor: (batch,): kl between each sample
    """
    element_wise = 0.5 * (torch.log(pv) - torch.log(qv) + qv / pv + (qm - pm).pow(2) / pv - 1)
    kl = element_wise.sum(-1)
    # print("log var1", qv)
    return kl

  def KL_loss(self, mu, log_var, z):

    kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())
    kl = kl.sum(-1)  # to go from multi-dimensional z to single dimensional z : (batch_size x latent_size) ---> (batch_size)
                                                                      # i.e Z = [ [z1_1, z1_2 , ...., z1_lt] ] ------> z = [ z1]
                                                                      #         [ [z2_1, z2_2, ....., z2_lt] ]             [ z2]
                                                                      #                   .                                [ . ]
                                                                      #                   .                                [ . ]
                                                                      #         [[zn_1, zn_2, ....., zn_lt] ]              [ zn]

                                                                      #        lt=latent_size
    kl = kl.mean()

    return kl

  def reconstruction_loss(self, x_hat_param, x):


    x = x.view(-1).contiguous()
    x_hat_param = x_hat_param.view(-1, x_hat_param.size(2))

    recon = self.nlloss(x_hat_param, x)

    return recon


  def forward(self, mu, log_var,z, x_hat_param, x):
    kl_loss = self.KL_loss(mu, log_var, z)
    recon_loss = self.reconstruction_loss(x_hat_param, x)


    elbo = kl_loss + recon_loss # we use + because recon loss is a NLLoss (cross entropy) and it's negative in its own, and in the ELBO equation we have
                              # elbo = KL_loss - recon_loss, therefore, ELBO = KL_loss - (NLLoss) = KL_loss + NLLoss

    return elbo, kl_loss, recon_loss

import torch
import time

class Trainer:

    def __init__(self, train_loader, test_loader, model, loss, optimizer, latent_dim, z1_dim=5) -> None:
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.model = model
        self.loss = loss
        self.optimizer = optimizer
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.interval = 200
        self.scale = np.array([[0,5],[0,5],[0.0, 1.0],[0.0,1.0],[0.0,1.0]])
        self.latent_dim = latent_dim
        self.z1_dim = z1_dim

    def get_batch(self, batch):
      sentences = batch["input"]
      target = batch["target"]
      sentences_length = batch["length"]
      label = batch["label"]
      return sentences, target, sentences_length, label

    def train(self, train_losses, epoch, batch_size, clip) -> list:
        # Initialization of RNN hidden, and cell states.
        states = self.model.init_hidden(batch_size)

        for batch_num, batch in enumerate(self.train_loader): # loop over the data, and jump with step = bptt.
            # get the labels

            source, target, source_lengths, label = self.get_batch(batch)
            source = source.to(self.device)
            target = target.to(self.device)
            # print("label ",label.size())

            x_hat_param, mu, mu_masked, log_var, z, z_masked, states, causal_matrix = self.model(source,source_lengths, states) # forward # x_hat, mean, mean_masked, log_var, z, z_masked, hidden_encoder

            # detach hidden states
            states = states[0].detach(), states[1].detach()

            ### start SCM implemtation ###

            # set causal priors

            p_m, p_v = torch.zeros(mu.size()), torch.ones(log_var.size()) #unit gaussian
            cp_m, cp_v = self.structural_condition_prior(self.scale, label, self.z1_dim, causal_matrix)


            # cp_v = torch.ones([mu.size()[0], self.z1_dim, self.z1_dim]).to(device)
            # cp_z = cp_m + (cp_v ** 0.5) * (torch.randn(cp_m.size()).to(device)) # sample by adding noise

            # KL-DIVERGENCE BETWEEN DISTRIBUTION FROM ENCODER AND THE ISOTROPIC GAUSSIAN PRIOR
            kl = torch.zeros(1).to(device)

            # RESHAPE
            z_m = mu.view(-1, self.latent_dim).to(device)
            z_v = log_var.view(-1, self.latent_dim).to(device)
            p_m = p_m.view(-1, self.latent_dim).to(device)
            p_v = p_v.view(-1, self.latent_dim).to(device)

            alpha=0.3
            beta=1

            kl = alpha * self.loss.kl_normal(z_m, z_v, p_m, p_v)

            for i in range(self.z1_dim):
              kl = kl + beta * self.loss.kl_normal(z_masked[:,:, i, :].to(device), cp_v[:, i, :].to(device),
                            cp_m[:, i, :].to(device), cp_v[:, i, :].to(device))

            kl = torch.mean(kl)

            # compute the loss
            mloss, KL_loss, recon_loss = self.loss(mu = mu, log_var = log_var, z = z, x_hat_param = x_hat_param , x = target)

            mloss_new = recon_loss + kl

            train_losses.append((mloss_new , kl.item(), recon_loss.item()))

            ### end SCM imlementation ##

            mloss.backward()

            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)

            self.optimizer.step()

            self.optimizer.zero_grad()


            if batch_num % self.interval == 0 and batch_num > 0:

                print('| epoch {:3d} | elbo_loss {:5.6f} | kl_loss {:5.6f} | recons_loss {:5.6f} '.format(
                    epoch, mloss.item(), KL_loss.item(), recon_loss.item()))

        return train_losses

    def test(self, test_losses, epoch, batch_size) -> list:

        with torch.no_grad():

            states = self.model.init_hidden(batch_size)

            for batch_num, batch in enumerate(self.test_loader): # loop over the data, and jump with step = bptt.
                # get the labels
                source, target, source_lengths, label = self.get_batch(batch)
                source = source.to(self.device)
                target = target.to(self.device)


                x_hat_param, mu, mu_masked, log_var, z, z_masked, states, causal_matrix = self.model(source,source_lengths, states)
                # print("source", source)
                # print("source size", source.size())
                # print("xhat", x_hat_param)
                # print("xhat size", x_hat_param.size())
                _, s = torch.topk(x_hat_param, 1)
                # print("s=", s.size())
                # print(s)
                s = s.view(s.size()[0], -1)
                sentences = []
                for row in range(s.size()[0]):
                  sample = s[row]

                  w_sample = [self.model.dictionary.get_i2w()[str(idx.item())] for idx in sample]
                  print(w_sample)
                  sentences.append(w_sample)

                # detach hidden states
                states = states[0].detach(), states[1].detach()


                ### start SCM implemtation ###

                # set causal priors

                p_m, p_v = torch.zeros(mu.size()), torch.ones(log_var.size()) #unit gaussian
                cp_m, cp_v = self.structural_condition_prior(self.scale, label, self.z1_dim, causal_matrix)

                kl = torch.zeros(1).to(device)
                            # RESHAPE
                z_m = mu.view(-1, self.latent_dim).to(device)
                z_v = log_var.view(-1, self.latent_dim).to(device)
                p_m = p_m.view(-1, self.latent_dim).to(device)
                p_v = p_v.view(-1, self.latent_dim).to(device)

                alpha=0.3
                beta=1

                kl = alpha * self.loss.kl_normal(z_m, z_v, p_m, p_v)

                for i in range(self.z1_dim):
                  kl = kl + beta * self.loss.kl_normal(z_masked[:,:, i, :].to(device), cp_v[:, i, :].to(device),
                                cp_m[:, i, :].to(device), cp_v[:, i, :].to(device))

                kl = torch.mean(kl)

                # compute the loss
                mloss, KL_loss, recon_loss = self.loss(mu = mu, log_var = log_var, z = z, x_hat_param = x_hat_param , x = target)

                mloss_new = recon_loss + kl
                test_losses.append((mloss_new , KL_loss.item(), recon_loss.item()))

                # Statistics.
                # if batch_num % 20 ==0:
                #   print('| epoch {:3d} | elbo_loss {:5.6f} | kl_loss {:5.6f} | recons_loss {:5.6f} '.format(
                #         epoch, mloss.item(), KL_loss.item(), recon_loss.item()))

            return test_losses

    def test_intervention(self, test_losses, epoch, batch_size, mask, value) -> list:

        with torch.no_grad():

            states = self.model.init_hidden(batch_size)

            for batch_num, batch in enumerate(self.test_loader): # loop over the data, and jump with step = bptt.
                # get the labels
                source, target, source_lengths, label = self.get_batch(batch)
                source = source.to(self.device)
                target = target.to(self.device)


                x_hat_param, mu, mu_masked, log_var, z, z_masked, states, causal_matrix = self.model(source,source_lengths, states, label, mask, value)


                _, s = torch.topk(x_hat_param, 1)
                s = s.view(s.size()[0], -1)
                sentences = []
                for row in range(s.size()[0]):
                  sample = s[row]
                  w_sample = [self.model.dictionary.get_i2w()[str(idx.item())] for idx in sample]
                  print(w_sample)
                  sentences.append(w_sample)


                # detach hidden states
                states = states[0].detach(), states[1].detach()


                ### start SCM implemtation ###

                # set causal priors

                p_m, p_v = torch.zeros(mu.size()), torch.ones(log_var.size()) #unit gaussian
                cp_m, cp_v = self.structural_condition_prior(self.scale, label, self.z1_dim, causal_matrix)

                kl = torch.zeros(1).to(device)
                            # RESHAPE
                z_m = mu.view(-1, self.latent_dim).to(device)
                z_v = log_var.view(-1, self.latent_dim).to(device)
                p_m = p_m.view(-1, self.latent_dim).to(device)
                p_v = p_v.view(-1, self.latent_dim).to(device)

                alpha=0.3
                beta=1

                kl = alpha * self.loss.kl_normal(z_m, z_v, p_m, p_v)

                for i in range(self.z1_dim):
                  kl = kl + beta * self.loss.kl_normal(z_masked[:,:, i, :].to(device), cp_v[:, i, :].to(device),
                                cp_m[:, i, :].to(device), cp_v[:, i, :].to(device))

                kl = torch.mean(kl)

                # compute the loss
                mloss, KL_loss, recon_loss = self.loss(mu = mu, log_var = log_var, z = z, x_hat_param = x_hat_param , x = target)

                mloss_new = recon_loss + kl
                test_losses.append((mloss_new , KL_loss.item(), recon_loss.item()))

                # Statistics.
                # if batch_num % 20 ==0:
                #   print('| epoch {:3d} | elbo_loss {:5.6f} | kl_loss {:5.6f} | recons_loss {:5.6f} '.format(
                #         epoch, mloss.item(), KL_loss.item(), recon_loss.item()))

            return test_losses

    def structural_condition_prior(self, scale, label, dim, causal_matrix):
      A = causal_matrix
      mean = torch.ones(label.size()[0], label.size()[1], dim)
      var = torch.ones(label.size()[0], label.size()[1], dim)
      I = torch.eye(5).to(device)
      for i in range(label.size()[0]):
        for j in range(label.size()[1]):
          inp = A.to(device).t() + I
          num_parents = torch.count_nonzero((inp), dim=1)
          norm_label = (label[i].to(device) - torch.tensor(scale[:, 0]).to(device)) / (
            torch.tensor(scale[:, 1]).to(device))
          mul = torch.matmul((inp).float(), norm_label.float().to(device))[j]
          mul = mul / num_parents[j]  # averaging

          mean[i][j] = torch.ones(dim) * mul.item()

          var[i][j] = torch.ones(dim) * 1
          return mean, var

batch_size = 30

train_loader = torch.utils.data.DataLoader( dataset= train_dataset, batch_size=batch_size, shuffle= True)
test_loader = torch.utils.data.DataLoader( dataset= test_dataset, batch_size= batch_size, shuffle= True)

demo_train_loader = torch.utils.data.DataLoader( dataset= demo_train_dataset, batch_size= batch_size, shuffle= True)
demo_test_loader = torch.utils.data.DataLoader( dataset= demo_test_dataset, batch_size= batch_size, shuffle= True)

vocab_size = train_dataset.vocab_size

device = "cuda" if torch.cuda.is_available() else "cpu"
torch.manual_seed(111)

embed_size = 300
hidden_size = 256
latent_size = 25#16
# model_name, z_dim, z1_dim, z2_dim, vocab_size, embed_size
model = Causal_LSTM_VAE(model_name="Causal_LSTM_VAE",vocab_size = vocab_size, embed_size = embed_size, hidden_size = hidden_size, latent_size = latent_size,batch_size=batch_size).to(device)

Loss = VAE_Loss()
learning_rate = 0.001
optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)

trainer = Trainer(train_loader, test_loader, model, Loss, optimizer, latent_dim=latent_size)

train_losses = []
test_losses = []
clip = 0.25
for epoch in range(1):
    print("Epoch: ", epoch)
    print("Training.......")
    train_losses = trainer.train(train_losses, epoch, batch_size, clip)
    # print("Testing.......")
    # test_losses = trainer.test(test_losses, epoch, batch_size)

PATH = "/content/drive/My Drive/Colab Notebooks/causality/dataset/langSCM.pt"
torch.save(model, PATH)
saved_model =  torch.load(PATH)

int_losses = []
int_losses = trainer.test_intervention(int_losses, epoch, batch_size, 0, 3)

demo_test_data



import matplotlib.pyplot as plt
def plot_elbo(losses, mode):
    elbo_loss = list(map(lambda x: x[0], losses))
    kl_loss = list(map(lambda x: x[1], losses))
    recon_loss = list(map(lambda x: x[2], losses))

    losses = {"elbo": elbo_loss, "kl": kl_loss, "recon": recon_loss}
    print(losses)

    for key in losses.keys():
        loss_np = []
        for loss in losses.get(key):
          if isinstance(loss, (float)):
            loss = loss
          else:
            loss = loss.detach().numpy()
          loss_np.append(loss)
        plt.plot(loss_np, label=key+"_" + mode)

    plt.legend()
    plt.show()

def interpolate(model, n_interpolations, sos, sequence_length):

  # # Get input.

  z1 = torch.randn((1,1,latent_size)).to(device)
  z2 = torch.randn((1,1,latent_size)).to(device)

  text1 = model.inference(sequence_length , sos, z1)
  text2 = model.inference(sequence_length , sos, z2)

  alpha_s = torch.linspace(0,1,n_interpolations)

  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])


  samples = [model.inference(sequence_length , sos, z) for z in interpolations]




  return samples, text1, text2



z1 = torch.randn(1,1,latent_size).to(device)
z2 = torch.randn(1,1,latent_size).to(device)

sos = "wife"
sample1 = saved_model.inference(10 , sos, z1)
sample2 = saved_model.inference(10 , sos , z2)


print(sample1)
print(sample2)

samples, text1, text2 = interpolate(model, 2,"game", 10)
print("First sentence:", text1)
print("Second sentence:", text2)

for sample in samples: print(sample)

label = torch.rand(3, 5)
print("label", label)
scale = np.array([[0,5],[0,5],[0.0, 1.0],[0.0,1.0],[0.0,1.0]])
dim =5
mean = torch.ones(label.size()[0], label.size()[1], dim)
var = torch.ones(label.size()[0], label.size()[1], dim)

I = torch.eye(5).to(device)
A = torch.zeros(5, 5)
A[0, 2:5] = 1
A[1, 2:5] = 1

print("original",mean)
for i in range(label.size()[0]):
  for j in range(label.size()[1]):
    inp = A.to(device).t() + I

    # print("inp", inp)
    num_parents = torch.count_nonzero((inp), dim=1)
    # print("num parents", num_parents)
    norm_label = (label[i].to(device) - torch.tensor(scale[:, 0]).to(device)) / (
      torch.tensor(scale[:, 1]).to(device))
    # print("label", label[i])
    # print("norm_label", norm_label)
    mul = torch.matmul((inp).float(), norm_label.float().to(device))[j]
    # print("mul", mul)
    mul = mul / num_parents[j]  # averaging
    # print("mul", mul)
    mean[i][j] = torch.ones(dim) * mul.item()
    # print(mean[i][j])
    var[i][j] = torch.ones(dim) * 1

print("modified",mean)
print(mean.size())